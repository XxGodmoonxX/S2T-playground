import { useState, useRef, useCallback, useEffect } from 'react';

export interface RealtimeState {
  isConnected: boolean;
  isRecording: boolean;
  isTranscribing: boolean;
  currentTranscription: string;
  finalTranscriptions: string[];
  error: string | null;
}

export const useRealtimeTranscription = () => {
  const [state, setState] = useState<RealtimeState>({
    isConnected: false,
    isRecording: false,
    isTranscribing: false,
    currentTranscription: '',
    finalTranscriptions: [],
    error: null,
  });

  const wsRef = useRef<WebSocket | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  // WebSocketÊé•Á∂ö„ÇíÁ¢∫Á´ã
  const connect = useCallback(async (apiKey: string) => {
    try {
      setState((prev: RealtimeState) => ({ ...prev, error: null }));
      
      const wsUrl = `wss://api.openai.com/v1/realtime?intent=transcription`;
      const ws = new WebSocket(wsUrl);

      ws.onopen = () => {
        console.log('üîó Realtime APIÊé•Á∂öÊàêÂäü');
        setState(prev => ({ ...prev, isConnected: true }));
        
        // „Çª„ÉÉ„Ç∑„Éß„É≥Ë®≠ÂÆö„ÇíÈÄÅ‰ø°
        const sessionConfig = {
          type: 'transcription_session.update',
          session: {
            input_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'gpt-4o-transcribe',
              language: 'ja',
            },
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 1000,
            },
            input_audio_noise_reduction: {
              type: 'near_field',
            },
          }
        };
        
        ws.send(JSON.stringify(sessionConfig));
      };

      ws.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          handleWebSocketMessage(message);
        } catch (error) {
          console.error('WebSocket„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆËß£Êûê„Ç®„É©„Éº:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocket„Ç®„É©„Éº:', error);
        setState(prev => ({ 
          ...prev, 
          error: 'WebSocketÊé•Á∂ö„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü',
          isConnected: false 
        }));
      };

      ws.onclose = () => {
        console.log('üîå WebSocketÊé•Á∂ö„ÅåÈñâ„Åò„Çâ„Çå„Åæ„Åó„Åü');
        setState(prev => ({ 
          ...prev, 
          isConnected: false,
          isRecording: false,
          isTranscribing: false
        }));
      };

      wsRef.current = ws;
    } catch (error) {
      setState(prev => ({ 
        ...prev, 
        error: error instanceof Error ? error.message : 'WebSocketÊé•Á∂ö„Å´Â§±Êïó„Åó„Åæ„Åó„Åü'
      }));
    }
  }, []);

  // WebSocket„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆÂá¶ÁêÜ
  const handleWebSocketMessage = useCallback((message: any) => {
    switch (message.type) {
      case 'transcription_session.created':
        console.log('‚úÖ ÊñáÂ≠óËµ∑„Åì„Åó„Çª„ÉÉ„Ç∑„Éß„É≥‰ΩúÊàêÂÆå‰∫Ü');
        break;
        
      case 'transcription_session.updated':
        console.log('‚úÖ ÊñáÂ≠óËµ∑„Åì„Åó„Çª„ÉÉ„Ç∑„Éß„É≥Êõ¥Êñ∞ÂÆå‰∫Ü');
        break;
        
      case 'input_audio_buffer.speech_started':
        console.log('üé§ Èü≥Â£∞Ê§úÂá∫ÈñãÂßã');
        setState(prev => ({ ...prev, isTranscribing: true }));
        break;
        
      case 'input_audio_buffer.speech_stopped':
        console.log('üîá Èü≥Â£∞Ê§úÂá∫ÂÅúÊ≠¢');
        setState(prev => ({ ...prev, isTranscribing: false }));
        break;
        
      case 'conversation.item.input_audio_transcription.delta':
        const delta = message.delta || '';
        setState(prev => ({ 
          ...prev, 
          currentTranscription: prev.currentTranscription + delta 
        }));
        break;
        
      case 'conversation.item.input_audio_transcription.completed':
        const transcript = message.transcript || '';
        setState(prev => ({ 
          ...prev, 
          finalTranscriptions: [...prev.finalTranscriptions, transcript],
          currentTranscription: ''
        }));
        break;
        
      case 'error':
        console.error('Realtime API„Ç®„É©„Éº:', message.error);
        setState(prev => ({ 
          ...prev, 
          error: `API Error: ${message.error?.message || 'Unknown error'}` 
        }));
        break;
        
      default:
        console.log('Âèó‰ø°„É°„ÉÉ„Çª„Éº„Ç∏:', message.type);
    }
  }, []);

  // Èå≤Èü≥ÈñãÂßã
  const startRecording = useCallback(async () => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      setState(prev => ({ ...prev, error: 'WebSocketÊé•Á∂ö„ÅåÁ¢∫Á´ã„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì' }));
      return;
    }

    try {
      // „Éû„Ç§„ÇØ„Ç¢„ÇØ„Çª„Çπ„ÇíÂèñÂæó
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        }
      });

      streamRef.current = stream;
      
      // AudioContext„ÇíË®≠ÂÆö
      audioContextRef.current = new AudioContext({ sampleRate: 24000 });
      const source = audioContextRef.current.createMediaStreamSource(stream);
      
      // ScriptProcessorNode„Åß„É™„Ç¢„É´„Çø„Ç§„É†Èü≥Â£∞Âá¶ÁêÜ
      processorRef.current = audioContextRef.current.createScriptProcessor(1024, 1, 1);
      
      processorRef.current.onaudioprocess = (event) => {
        if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return;
        
        const inputBuffer = event.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        
        // Float32Array„ÇíPCM16„Å´Â§âÊèõ
        const pcm16Data = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          pcm16Data[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
        }
        
        // Base64„Ç®„É≥„Ç≥„Éº„Éâ
        const audioData = Array.from(new Uint8Array(pcm16Data.buffer));
        const base64Data = btoa(String.fromCharCode(...audioData));
        
        // Realtime API„Å´ÈÄÅ‰ø°
        const audioMessage = {
          type: 'input_audio_buffer.append',
          audio: base64Data
        };
        
        wsRef.current.send(JSON.stringify(audioMessage));
      };
      
      source.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);
      
      setState(prev => ({ ...prev, isRecording: true }));
      console.log('üéôÔ∏è „É™„Ç¢„É´„Çø„Ç§„É†Èå≤Èü≥ÈñãÂßã');
      
    } catch (error) {
      setState(prev => ({ 
        ...prev, 
        error: '„Éû„Ç§„ÇØ„Ç¢„ÇØ„Çª„Çπ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ„Éû„Ç§„ÇØ„ÅÆ‰ΩøÁî®„ÇíË®±ÂèØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ' 
      }));
    }
  }, []);

  // Èå≤Èü≥ÂÅúÊ≠¢
  const stopRecording = useCallback(() => {
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }
    
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    setState(prev => ({ 
      ...prev, 
      isRecording: false,
      isTranscribing: false
    }));
    
    console.log('üéôÔ∏è „É™„Ç¢„É´„Çø„Ç§„É†Èå≤Èü≥ÂÅúÊ≠¢');
  }, []);

  // Êé•Á∂öÂàáÊñ≠
  const disconnect = useCallback(() => {
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }
    
    stopRecording();
    
    setState(prev => ({ 
      ...prev, 
      isConnected: false,
      currentTranscription: '',
      finalTranscriptions: []
    }));
  }, [stopRecording]);

  // „É™„Çª„ÉÉ„Éà
  const reset = useCallback(() => {
    setState(prev => ({ 
      ...prev,
      currentTranscription: '',
      finalTranscriptions: [],
      error: null
    }));
  }, []);

  // „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    ...state,
    connect,
    disconnect,
    startRecording,
    stopRecording,
    reset,
  };
};